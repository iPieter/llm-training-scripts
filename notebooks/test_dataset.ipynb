{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jsonlines_dir(path):\n",
    "    data_files = glob.glob(path)\n",
    "    print(f\"Number of files {len(data_files)} after adding {path}\")\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files 4 after adding ../export/enwiki_*_validation.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "validation = add_jsonlines_dir(f\"../export/enwiki_*_validation.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files 3497 after adding ../export/enwiki_*_train.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "train = add_jsonlines_dir(f\"../export/enwiki_*_train.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('json', data_files={'train': train, 'validation': validation}, cache_dir='/cw/dtaijupiter/NoCsBack/dtai/pieterd/hf_cache', streaming=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterator = iter(dataset['validation']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_articles = {t['title'] for t in dataset_iterator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30840"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_articles.txt', 'w') as f:\n",
    "    for item in validation_articles:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IterableDatasetDict' object has no attribute 'push_to_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdelobelle/enwiki-yearly-cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m, use_temp_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, private\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IterableDatasetDict' object has no attribute 'push_to_hub'"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub('pdelobelle/enwiki-yearly-cleaned', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/pieterd/.cache/huggingface/modules/datasets_modules/datasets/pdelobelle--enwiki-yearly-cleaned/866094395100db4342b850568c3718e6181ff3c000d14a78e75e441e571b281e (last modified on Wed Jul 24 17:12:01 2024) since it couldn't be found locally at pdelobelle/enwiki-yearly-cleaned, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HfFileSystemFile' object has no attribute 'resolved_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m iterable_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m shuffled_iterable_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterable_dataset\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshuffled_iterable_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/datasets/iterable_dataset.py:1388\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m ex_iterable:\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m   1390\u001b[0m         \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m         \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m         example \u001b[38;5;241m=\u001b[39m _apply_feature_types_on_example(\n\u001b[1;32m   1393\u001b[0m             example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id\n\u001b[1;32m   1394\u001b[0m         )\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/datasets/iterable_dataset.py:987\u001b[0m, in \u001b[0;36mBufferShuffledExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# this is the shuffle buffer that we keep in memory\u001b[39;00m\n\u001b[1;32m    986\u001b[0m mem_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mem_buffer) \u001b[38;5;241m==\u001b[39m buffer_size:  \u001b[38;5;66;03m# if the buffer is full, pick and example from it\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(indices_iterator)\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/datasets/iterable_dataset.py:262\u001b[0m, in \u001b[0;36mShuffledDataSourcesExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m rng \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator)\n\u001b[1;32m    261\u001b[0m kwargs_with_shuffled_shards \u001b[38;5;241m=\u001b[39m _shuffle_gen_kwargs(rng, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_examples_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_with_shuffled_shards)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/pdelobelle--enwiki-yearly-cleaned/866094395100db4342b850568c3718e6181ff3c000d14a78e75e441e571b281e/enwiki-yearly-cleaned.py:135\u001b[0m, in \u001b[0;36mWikipedia._generate_examples\u001b[0;34m(self, filepaths)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m files \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper(filepaths, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    134\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating examples from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m     gzip_iters \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgzip_open(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lines \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgzip_iters):\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/pdelobelle--enwiki-yearly-cleaned/866094395100db4342b850568c3718e6181ff3c000d14a78e75e441e571b281e/enwiki-yearly-cleaned.py:135\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m files \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper(filepaths, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    134\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating examples from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m     gzip_iters \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgzip_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lines \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgzip_iters):\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/pdelobelle--enwiki-yearly-cleaned/866094395100db4342b850568c3718e6181ff3c000d14a78e75e441e571b281e/enwiki-yearly-cleaned.py:128\u001b[0m, in \u001b[0;36mWikipedia.gzip_open\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgzip_open\u001b[39m(filepath):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/datasets/streaming.py:75\u001b[0m, in \u001b[0;36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:512\u001b[0m, in \u001b[0;36mxopen\u001b[0;34m(file, mode, download_config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m \u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot seek streaming HTTP file\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/fsspec/core.py:132\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/fsspec/core.py:100\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     98\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 100\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobjects \u001b[38;5;241m=\u001b[39m [f]\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/fsspec/spec.py:1307\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1307\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:228\u001b[0m, in \u001b[0;36mHfFileSystem._open\u001b[0;34m(self, path, mode, revision, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HfFileSystemStreamFile(\u001b[38;5;28mself\u001b[39m, path, mode\u001b[38;5;241m=\u001b[39mmode, revision\u001b[38;5;241m=\u001b[39mrevision, block_size\u001b[38;5;241m=\u001b[39mblock_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHfFileSystemFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cw/dtaijupiter/NoCsBack/dtai/pieterd/projects/llm-training/.env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:621\u001b[0m, in \u001b[0;36mHfFileSystemFile.__init__\u001b[0;34m(self, fs, path, revision, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    619\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure the repository and revision exist before writing data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(fs, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241m.\u001b[39munresolve(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs: HfFileSystem\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HfFileSystemFile' object has no attribute 'resolved_path'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset('pdelobelle/enwiki-yearly-cleaned', 'tiny', streaming=True, download_mode='force_redownload')\n",
    "iterable_dataset = dataset['train']\n",
    "shuffled_iterable_dataset = iter(iterable_dataset.shuffle(seed=42, buffer_size=1000))\n",
    "\n",
    "next(shuffled_iterable_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '22035256',\n",
       " 'text': 'Martin Welz is a South African journalist and the editor of Noseweek magazine. He is best known for his investigative work on controversial issues including government and corporate corruption.\\n\\n== Early life ==\\nMartin Sylvester Welz was born on 19 October 1945 in Worcester, Western Cape, South Africa. Fourth son of artist Jean Welz, born Salzburg, Austria 1900, and Inger Marie Welz, née Christensen, born in Odense, Denmark 1908.\\n\\n== Career == \\n\\n === Sunday Times ===\\nA report by Welz while he was working for the Sunday Times from 1977–1981 saw a R180-million defamation claim instituted against himself and the paper by Lebanese businessman Salim el Hajj. El Hajj had been accused by Welz of a series of frauds in the then black \"homelands\". El Hajj fled the country before the case got to court. Welz also worked on South Africa\\'s \"Muldergate\" Information scandal, and helped expose wrongdoing by apartheid-era cabinet ministers, inter alia revealing that both Minister of Manpower and Energy Fanie Botha and State President Nico Diederichs were secretly bankrupt while in office. In 1981–82, Welz was appointed Parliamentary correspondent for Sunday Express, Johannesburg. While at the Express, he won the Stellenbosch farmers\\' Winery Award (1983) for a series exposing the corrupt pharmaceutical empire established by businessmen Isaac Kay and David Tabatznik.Welz, subsequently, when heading Rapport\\'s new investigations department, exposed a network of conmen dubbed the \"boere mafia\" and Adriaan Niewoudt\\'s Kubus milk culture pyramid scheme, later also exposed as the biggest US postal fraud in history. (Federal Court, Topeka, Kansas, 1986.) He was later appointed as a consultant to the United States Department of Justice and helped expose brothers Frans and Gert Theron as the Kubus masterminds in the US. \\n\\n=== Noseweek ===\\nNoseweek was founded in Cape Town, South Africa, in June 1993.  It contains a minimal amount of advertising and has survived and grown mainly on its sales.\\n\\n== Lawsuits == \\n\\n === Dr Robert Milton Hall ===\\nThe first lawsuit again the magazine was brought in 1994 by an American tax refugee and millionaire dentist living in Stellenbosch, Western Cape, called Dr Robert Milton Hall.  The trial was in 1996 in the Cape High Court before Judge Johann Conradie.  Noseweek had, inter alia, pointed out Dr Hall\\'s fraudulent claims to be the \"inventor of modern-day dentistry\", and his ongoing foreign exchange and tax contraventions.\\nThe magazine was represented by an attorney and advocates and Welz represented himself.  Noseweek and Welz won the case.  Judge Johann Conradie presided and in his judgment found that Dr Hall, \"... sued not to salvage his reputation but to sustain a colossal fraud.\"  This litigation put the magazine on ice for approximately two years and nearly bankrupted it. It was relaunched thanks to the voluntary contributions of scores of its loyal readers.\\n\\n=== FirstRand Bank ===\\nIn 2007 Welz represented himself in a court action brought by the FirstRand group to prevent Noseweek from publishing a list of FirstRand clients allegedly involved in a dubious offshore investment scheme. The action was dismissed with costs.\\n\\n=== Inge Peacock ===\\nCape Town businesswoman, Inge Peacock, sued Noseweek and Martin Welz in March 2012. Judge Andre Le Grange of the Cape High Court dismissed Peacock\\'s case with costs, but stated that the plaintiff may pursue damages for defamation against Noseweek.  The resulting publicity created a \"Streisand effect\", after the mainstream press picked up the story.\\n\\n==== Weapons whistleblower ====\\nIn 2008 Welz reported that a ship due to dock in Durban harbour carried a shipment of Chinese weapons bound for Zimbabwe.\\nNews of the $1.245 million, 77-ton shipment came via what Welz described as \"a whistle blower of conscience,\" who supplied Noseweek with a commercial invoice, bill of lading and packing list for the shipment.\\n\\n== Awards ==\\n* South African Union of Journalists Thomas Pringle Award for Press Freedom.\\n* Special mention, 2002 Nat Nakasa Award for Media Integrity & Courageous Journalism.\\n* Joint business category winner, 2004 Mondi Paper Magazine Awards.\\n* Honorary award for promoting corporate governance through investigative journalism, 2007 Sanlam Financial Journalist of the Year competition.\\n',\n",
       " 'year': '2020',\n",
       " 'tlsh': 'T12691A437979613770F4243D23B5E32E37B154A6D5251CA52A04C863D2349CBA8BBA7D4',\n",
       " 'title': 'Martin Welz'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
